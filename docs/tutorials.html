<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Tutorials &#8212; Robo API 1 documentation</title>
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script type="text/javascript" src="_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="APIs" href="apis.html" />
    <link rel="prev" title="Installation" href="installation.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          RoBO</a>
        <span class="navbar-text navbar-version pull-left"><b>0.2.0</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="index.html">Start</a></li>
                <li><a href="installation.html">Installation</a></li>
                <li><a href="#">Tutorial</a></li>
                <li><a href="apis.html">APIs</a></li>
                <li><a href="contact.html">Contact</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="example.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="contact.html">Contact</a></li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><ul>
<li><a class="reference internal" href="#">Tutorials</a><ul>
<li><a class="reference internal" href="#blackbox-function-optimization-with-robo">Blackbox function optimization with RoBO</a></li>
<li><a class="reference internal" href="#bohamiann">Bohamiann</a></li>
<li><a class="reference internal" href="#fabolas">Fabolas</a></li>
<li><a class="reference internal" href="#robo-on-hpolib2-benchmarks">RoBO on HPOLIB2 benchmarks</a></li>
<li><a class="reference internal" href="#fitting-a-bayesian-neural-network">Fitting a Bayesian neural network</a></li>
<li><a class="reference internal" href="#fanova">FANOVA</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    <div class="col-md-9 content">
      
  <a href="https://github.com/automl/RoBO"><img style="position: fixed; top: 50px; right: 0; border: 0;" src="https://camo.githubusercontent.com/365986a132ccd6a44c23a9169022c0b5c890c387/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f7265645f6161303030302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png"></a><div class="section" id="tutorials">
<h1>Tutorials<a class="headerlink" href="#tutorials" title="Permalink to this headline">¶</a></h1>
<p>The following tutorials will help you to get familiar with RoBO.
You can find the code for all the tutorials and additional examples
in the <code class="docutils literal notranslate"><span class="pre">examples</span></code> folder.</p>
<ol class="arabic simple">
<li>Blackbox function optimization with RoBO</li>
<li>Bohamiann</li>
<li>Fabolas</li>
<li>Multi-Task Bayesian optimization</li>
<li>RoBO on HPOLIB2 benchmarks</li>
<li>Fitting a Bayesian neural network</li>
</ol>
<div class="section" id="blackbox-function-optimization-with-robo">
<h2>Blackbox function optimization with RoBO<a class="headerlink" href="#blackbox-function-optimization-with-robo" title="Permalink to this headline">¶</a></h2>
<p>This tutorial will show you how to use standard Bayesian optimization with Gaussian processes and different acquisition functions to find the global minimizer of your (python) function. Note that RoBO so far only support continuous input space and it is not able to handle multi-objective functions.</p>
<p>The first thing we have to do is to import numpy (for the objective function) and the bayesian_optimization interface</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">robo.fmin</span> <span class="kn">import</span> <span class="n">bayesian_optimization</span>
</pre></div>
</div>
</div></blockquote>
<p>To use RoBO we have to define a function that symbolizes the objective function we want to minimize. Interface of objective function is fairly simple, it gets an d-dimensional vector x and returns the corresponding scalar function value.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">objective_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div></blockquote>
<p>Before we run Bayesian optimization we first have to define the lower and upper bound of our input search space. In this case our search space contains only one dimension, but Bayesian optimization is not restricted to that and normally works fine up to 10 (continuous) dimensions.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">])</span>
</pre></div>
</div>
</div></blockquote>
<p>Now we have everything we need and can now run Bayesian optimization for 50 iterations:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">bayesian_optimization</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>At the end we get a dictionary back with contains the following entries:</p>
<ul class="simple">
<li>“x_opt” : the best found data point</li>
<li>“f_opt” : the corresponding function value</li>
<li>“incumbents”: the incumbent (best found value) after each iteration</li>
<li>“incumbent_value”: the function values of the incumbents</li>
<li>“runtime”: the runtime in seconds after each iteration</li>
<li>“overhead”: the optimization overhead (i.e. time data we do not spend to evaluate the function) of each iteration</li>
<li>“X”: all data points that have been evaluated</li>
<li>“y”: the corresponding function evaluations</li>
</ul>
<p>By default RoBO uses Gaussian processes (with MCMC sampling to obtain the GP’s hyperparameters) and logarithmic expected improvement as acquisition function. If you would like to use a different acquisition function such as for instance the lower confidence bound you can simple :</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">bayesian_optimization</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">acquisition_func</span><span class="o">=</span><span class="s1">&#39;lcb&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>See the API documentation for different possible choices of acquisition functions.</p>
<p>If you want to have a deeper look what RoBO is doing under the hood you can activate RoBO’s logging mechanism by adding the following two lines on top of your python script:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Besides standard Bayesian optimization, RoBO also contains an interface for plain random search and entropy search by Hennig et. al. Both methods follow the exact same interface.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">robo.fmin</span> <span class="kn">import</span> <span class="n">entropy_search</span>
<span class="kn">from</span> <span class="nn">robo.fmin</span> <span class="kn">import</span> <span class="n">random_search</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">entropy_search</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">random_search</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="bohamiann">
<h2>Bohamiann<a class="headerlink" href="#bohamiann" title="Permalink to this headline">¶</a></h2>
<p>RoBO offers an simple interface for Bayesian Optimization with Hamiltonian Monte Carlo Artificial Neural Networks (BOHAMIANN) which was introduced by Sprigenberg et al.</p>
<p>If you want to use Bohamiann make sure that you have Lasagne and Theano installed:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install Lasagne
pip install theano
</pre></div>
</div>
</div></blockquote>
<p>and that the <a class="reference external" href="https://github.com/stokasto/sgmcmc">sgmcmc package</a> is in your PYTHONPATH:</p>
<p>The interface to Bohamiann is exactly the same as for the GP based Bayesian optimization interface:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">robo.fmin</span> <span class="kn">import</span> <span class="n">bohamiann</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">bohamiann</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>This will return a dictionary with the same meta information as described above.</p>
<p>&#64;inproceedings{springenberg-nips2016, booktitle = {Advances in Neural Information Processing Systems 29}, month = {December}, title = {Bayesian optimization with robust Bayesian neural networks}, author = {J. T. Springenberg and A. Klein and S.Falkner and F. Hutter}, year = {2016} }</p>
</div>
<div class="section" id="fabolas">
<h2>Fabolas<a class="headerlink" href="#fabolas" title="Permalink to this headline">¶</a></h2>
<p>The idea of Fabolas (Klein et al.) is to take the training data set size as an additional input into account that can be freely chosen during the optimization procedure but is fixed afterwards. The idea is to speed up the optimization by evaluating single configurations only on much cheaper subset and to extrapolate their performance on the full dataset.</p>
<p>By additionally modelling the cost of training single configurations, Fabolas uses the information gain per unit cost to pick and evaluate configurations on small subset of the training data that give the most information about the global minimum on the full dataset.</p>
<p>Now the objective function gets besides a configuration also the training dataset size as an additional input. After training the configuration on a subset of the training data it returns the validation error on the full validation data set as well as the time it took to train this configuration.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">robo.fmin</span> <span class="kn">import</span> <span class="n">fabolas</span>

<span class="k">def</span> <span class="nf">objective_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="c1"># Train your algorithm here with x on the dataset subset with length s</span>
    <span class="c1"># Estimate the validation error and the cost on the validation data set</span>
    <span class="k">return</span> <span class="n">validation_error</span><span class="p">,</span> <span class="n">cost</span>
</pre></div>
</div>
</div></blockquote>
<p>Additionally you have to define the bounds of the input space for the configurations and the minimum and maximum data set size.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">])</span>
<span class="n">upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">s_min</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">s_max</span> <span class="o">=</span> <span class="mi">50000</span>
</pre></div>
</div>
</div></blockquote>
<p>Then you can call Fabolas by:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">fabolas</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span>
                  <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span>
                  <span class="n">upper</span><span class="o">=</span><span class="n">upper</span><span class="p">,</span>
                  <span class="n">s_min</span><span class="o">=</span><span class="n">s_min</span><span class="p">,</span>
                  <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span>
                  <span class="n">num_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>You can find a full example for training a support vector machine on MNIST <a class="reference external" href="https://github.com/automl/RoBO/blob/master/examples/example_fabolas.py">here</a></p>
<p>&#64;article{klein-corr16, author = {A. Klein and S. Falkner and S. Bartels and P. Hennig and F. Hutter}, title = {Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets}, journal = corr, llvolume = {abs/1605.07079}, lurl = {<a class="reference external" href="http://arxiv.org/abs/1605.07079">http://arxiv.org/abs/1605.07079</a>}, year = {2016} }</p>
</div>
<div class="section" id="robo-on-hpolib2-benchmarks">
<h2>RoBO on HPOLIB2 benchmarks<a class="headerlink" href="#robo-on-hpolib2-benchmarks" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/automl/HPOlib2">HPOlib2</a> contains a set of benchmarks with an unified interface for hyperparameter optimization of machine learning algorithms. In the following example we want to assume the often used synthetic function branin. Make sure that you installed HPOlib2.</p>
<p>First we load the benchmark and get the bound of the configuration space</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">hpolib.benchmarks.synthetic_functions</span> <span class="kn">import</span> <span class="n">Branin</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">Branin</span><span class="p">()</span>
<span class="n">info</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">get_meta_information</span><span class="p">()</span>
<span class="n">bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;bounds&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div></blockquote>
<p>Than we can simply run RoBO by:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">bayesian_optimization</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>HPOlib2 allows to evaluate single configurations only subsets of the data which allows us to use Fabolas or MTBO. If want to use Fabolas to optimize let’s say a support vector machine on MNIST we first have to wrap the HPOlib2 benchmarks class in order to pass the correct ration of the dataset size:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">hpolib.benchmarks.ml.svm_benchmark</span> <span class="kn">import</span> <span class="n">SvmOnMnist</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">SvmOnMnist</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="n">dataset_fraction</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="n">s_max</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">objective_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dataset_fraction</span><span class="o">=</span><span class="n">dataset_fraction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;function_value&quot;</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;cost&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
<p>Than we can run Fabolas simply by:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">info</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">get_meta_information</span><span class="p">()</span>
<span class="n">bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;bounds&#39;</span><span class="p">])</span>
<span class="n">lower</span> <span class="o">=</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">upper</span> <span class="o">=</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">fabolas</span><span class="p">(</span><span class="n">objective_function</span><span class="o">=</span><span class="n">objective</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="n">upper</span><span class="p">,</span>
                  <span class="n">s_min</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">s_max</span><span class="o">=</span><span class="n">s_max</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">n_hypers</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">subsets</span><span class="o">=</span><span class="p">[</span><span class="mf">64.</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="fitting-a-bayesian-neural-network">
<h2>Fitting a Bayesian neural network<a class="headerlink" href="#fitting-a-bayesian-neural-network" title="Permalink to this headline">¶</a></h2>
<p>The following tutorial we will see how we can train a Bayesian neural networks with stochastic MCMC sampling on our dataset. Note all models in RoBO implement the same interface and you can easily replace the Bayesian neural network by another model (Gaussian processes, Random Forest, …).</p>
<p>Assume we collect some data point of a sinc function:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">robo.models.bnn</span> <span class="kn">import</span> <span class="n">BayesianNeuralNetwork</span>
<span class="kn">from</span> <span class="nn">robo.initial_design.init_random_uniform</span> <span class="kn">import</span> <span class="n">init_random_uniform</span>


<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sinc</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">-</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">init_random_uniform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">20</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>We can now create and train a neural network by:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">BayesianNeuralNetwork</span><span class="p">(</span><span class="n">sampling_method</span><span class="o">=</span><span class="s2">&quot;sghmc&quot;</span><span class="p">,</span>
                              <span class="n">l_rate</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">),</span>
                              <span class="n">mdecay</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                              <span class="n">burn_in</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span>
                              <span class="n">n_iters</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span>
                              <span class="n">precondition</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                              <span class="n">normalize_input</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                              <span class="n">normalize_output</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>After training we can use our model to predict the mean and variance for arbitrary test points:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>
<span class="n">mean_pred</span><span class="p">,</span> <span class="n">var_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="fanova">
<h2>FANOVA<a class="headerlink" href="#fanova" title="Permalink to this headline">¶</a></h2>
<p>In the following tutorial, we will show how to use RoBO with FANOVA (Functional Analysis of Variance). FANOVA is a framework used for quantifying the performance of functions. More information of FANOVA can be found <a class="reference external" href="https://automl.github.io/fanova/">here</a> .</p>
<p>For this example, let’s use Brannin as our objective function and random search as our optimization method. First we have to import all the necessary packages that we will need.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fanova</span> <span class="kn">import</span> <span class="n">fANOVA</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">robo.fmin</span> <span class="kn">import</span> <span class="n">random_search</span>
<span class="kn">from</span> <span class="nn">hpolib.benchmarks.synthetic_functions</span> <span class="kn">import</span> <span class="n">Branin</span>
<span class="kn">import</span> <span class="nn">fanova.visualizer</span>
</pre></div>
</div>
</div></blockquote>
<p>Now we declare the objective function and get the bounds of the configuration space.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">objective_function</span> <span class="o">=</span> <span class="n">Branin</span><span class="p">()</span>
<span class="n">info</span> <span class="o">=</span> <span class="n">objective_function</span><span class="o">.</span><span class="n">get_meta_information</span><span class="p">()</span>
<span class="n">bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;bounds&#39;</span><span class="p">])</span>
<span class="n">config_space</span> <span class="o">=</span> <span class="n">objective_function</span><span class="o">.</span><span class="n">get_configuration_space</span><span class="p">()</span>
</pre></div>
</div>
</div></blockquote>
<p>Now we apply random search</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">random_search</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>After getting the results, we create a FANOVA object and pass the results to it.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">fANOVA</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>To compute now the marginal of the first parameter, we can wrtie:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">quantify_importance</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="p">)))</span>
</pre></div>
</div>
</div></blockquote>
<p>To visualize the single and pairwise marginals, we have to create a visualizer object first containing the fanova object, configspace and directory.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vis</span> <span class="o">=</span> <span class="n">fanova</span><span class="o">.</span><span class="n">visualizer</span><span class="o">.</span><span class="n">Visualizer</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">config_space</span><span class="p">,</span> <span class="s2">&quot;./plots/&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>We can then plot single marginals by</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vis</span><span class="o">.</span><span class="n">plot_marginal</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Which should look like:</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="_sources/tutorials.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2017, AAD Group.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.4.<br/>
    </p>
  </div>
</footer>
  </body>
</html>